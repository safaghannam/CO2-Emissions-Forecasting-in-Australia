{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import tracemalloc\n",
        "from itertools import combinations\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_squared_log_error, median_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, GRU, SimpleRNN, Dense\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "import xgboost as xgb\n",
        "from scipy import stats\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "E3IZfjOQkmp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exploratory Data Analysis (EDA) â€“ COâ‚‚ Emissions"
      ],
      "metadata": {
        "id": "LPofepmlkPP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Raw Time Series Visualization"
      ],
      "metadata": {
        "id": "TN9pwAQKlCNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your data\n",
        "\n",
        "file = '/co2_preprocessed.xlsx'\n",
        "data = pd.read_excel(file)\n",
        "\n",
        "# Define your features\n",
        "feature_keys = [\n",
        "    \"co2\", \"Consumption\", \"total_ghg_excluding_lucf\", \"GDP\", \"Energy consumption\",\n",
        "    \"Electricity Supply\", \"Coal\", \"Gas\", \"Oil\", \"Transport\", \"AUS Energy Growth-QLD\",\n",
        "    \"Total generation\", \"Residential_Energy\", \"Commercial_Energy\",\n",
        "    \"AUS Energy Growth-Rest of Australia\", \"Renewables\", \"land_use_change_co2\",\n",
        "    \"Net exports\", \"Energy intensity\", \"Energy productivity \",\n",
        "    \"Renewables generation\", \"AUS Energy Growth-NT\", \"population\"\n",
        "]\n",
        "\n",
        "# Optional: use same titles as feature names (or customise them if needed)\n",
        "titles = feature_keys\n",
        "\n",
        "# Set the year as datetime index\n",
        "date_time_key = \"year\"\n",
        "data[date_time_key] = pd.to_datetime(data[date_time_key], format='%Y')\n",
        "data.set_index(date_time_key, inplace=True)\n",
        "\n",
        "# Color palette for plots\n",
        "colors = plt.cm.tab20.colors\n",
        "\n",
        "# Define visualization function with 3 subplots per row\n",
        "def show_raw_visualization(data):\n",
        "    time_data = data.index\n",
        "    n_features = len(feature_keys)\n",
        "    ncols = 3\n",
        "    nrows = (n_features // ncols) + (n_features % ncols > 0)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(22, nrows * 4), dpi=100, facecolor=\"w\", edgecolor=\"k\")\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, key in enumerate(feature_keys):\n",
        "        c = colors[i % len(colors)]\n",
        "        if key in data.columns:\n",
        "            t_data = data[key]\n",
        "            t_data.index = time_data\n",
        "            ax = t_data.plot(ax=axes[i], color=c, title=titles[i], rot=25)\n",
        "            ax.legend([titles[i]])\n",
        "        else:\n",
        "            print(f\"Column '{key}' not found in DataFrame. Skipping...\")\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "show_raw_visualization(data)"
      ],
      "metadata": {
        "id": "NUZvlHEYzofE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter Plots â€“ COâ‚‚ vs Features"
      ],
      "metadata": {
        "id": "JTUDU5C3liEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set 'year' as datetime index\n",
        "data[\"year\"] = pd.to_datetime(data[\"year\"], format=\"%Y\")\n",
        "data.set_index(\"year\", inplace=True)\n",
        "\n",
        "# Define features (excluding co2 from the list of \"others\")\n",
        "feature_keys = [\n",
        "    \"Consumption\", \"total_ghg_excluding_lucf\", \"GDP\", \"Energy consumption\",\n",
        "    \"Electricity Supply\", \"Coal\", \"Gas\", \"Oil\", \"Transport\", \"AUS Energy Growth-QLD\",\n",
        "    \"Total generation\", \"Residential_Energy\", \"Commercial_Energy\",\n",
        "    \"AUS Energy Growth-Rest of Australia\", \"Renewables\", \"land_use_change_co2\",\n",
        "    \"Net exports\", \"Energy intensity\", \"Energy productivity \",\n",
        "    \"Renewables generation\", \"AUS Energy Growth-NT\", \"population\"\n",
        "]\n",
        "\n",
        "# Plot settings\n",
        "ncols = 3\n",
        "nrows = (len(feature_keys) // ncols) + (len(feature_keys) % ncols > 0)\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(22, nrows * 6))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot co2 vs each feature (scatter only)\n",
        "for i, feature in enumerate(feature_keys):\n",
        "    if \"co2\" in data.columns and feature in data.columns:\n",
        "        sns.scatterplot(data=data, x=feature, y=\"co2\", ax=axes[i], color='royalblue')\n",
        "        axes[i].set_title(f\"COâ‚‚ vs {feature}\", fontsize=12)\n",
        "        axes[i].set_xlabel(feature)\n",
        "        axes[i].set_ylabel(\"COâ‚‚\")\n",
        "    else:\n",
        "        axes[i].axis('off')\n",
        "        axes[i].text(0.5, 0.5, f\"Missing: co2 or {feature}\", ha='center')\n",
        "\n",
        "# Hide any extra subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ic_AnYwAPwGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histograms of Features"
      ],
      "metadata": {
        "id": "XTe2i5yKl5TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histograms for all features\n",
        "def plot_histograms(data):\n",
        "    fig, axes = plt.subplots(nrows=nrows_hist, ncols=ncols, figsize=(22, nrows_hist * 5), dpi=100, facecolor=\"w\", edgecolor=\"k\")\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, key in enumerate(feature_keys):\n",
        "        if key in data.columns:\n",
        "            ax = axes[i]\n",
        "            data[key].plot(kind='hist', bins=20, ax=ax, color=\"skyblue\", edgecolor=\"black\", alpha=0.7)\n",
        "            ax.set_title(f'Histogram: {key}', fontsize=12)\n",
        "            ax.set_xlabel(key)\n",
        "            ax.set_ylabel('Frequency')\n",
        "        else:\n",
        "            print(f\"Feature '{key}' not found in DataFrame. Skipping...\")\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot joint distributions for all features\n",
        "def plot_joint_distributions(data):\n",
        "    fig, axes = plt.subplots(nrows=nrows_joint, ncols=ncols, figsize=(22, nrows_joint * 5), dpi=100, facecolor=\"w\", edgecolor=\"k\")\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, key in enumerate(feature_keys):\n",
        "        if key in data.columns:\n",
        "            ax = axes[i]\n",
        "            sns.jointplot(x=key, y=key, data=data, kind=\"scatter\", color=\"skyblue\", height=5)\n",
        "            plt.suptitle(f'Joint Distribution: {key}', fontsize=12, y=1.02)\n",
        "        else:\n",
        "            print(f\"Feature '{key}' not found in DataFrame. Skipping...\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the functions to plot histograms and joint distributions\n",
        "plot_histograms(data)\n",
        "plot_joint_distributions(data)"
      ],
      "metadata": {
        "id": "xsYoIu2YQLaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#COâ‚‚ Forecasting: Data & Models\n",
        "\n",
        "In this section, we:\n",
        "\n",
        "- Import all necessary libraries for data processing, machine learning, and deep learning.\n",
        "- Load and preprocess COâ‚‚ data from 1982â€“2015, including 22 key features like energy, economic, population, and land-use factors.\n",
        "- Log-transform and clean the data for modeling.\n",
        "- Define evaluation metrics to track model performance (RMSE, RÂ², MAE, etc.).\n",
        "- Implement a custom Extreme Learning Machine (ELM) regressor.\n",
        "- Train and test 13 models including Random Forest, XGBoost, SVR, ARIMA, neural networks, and hybrid stacking models across 30 seeds.\n",
        "- Record runtime, memory usage, and hyperparameters.\n",
        "- Perform basic statistical tests to compare model performance.\n",
        "\n",
        "This sets up everything needed to train and evaluate the COâ‚‚ forecasting models.\n"
      ],
      "metadata": {
        "id": "6HHt2aVwmX3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import tracemalloc\n",
        "from itertools import combinations\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_squared_log_error, median_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, GRU, SimpleRNN, Dense\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "import xgboost as xgb\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ------------------ Load Data for years 1982 to 2015 ------------------ #\n",
        "df = pd.read_excel('/content/co2_training.xlsx')\n",
        "\n",
        "features = [\n",
        "    \"Consumption\", \"total_ghg_excluding_lucf\", \"GDP\", \"Energy consumption\",\n",
        "    \"Electricity Supply\", \"Coal\", \"Gas\", \"Oil\", \"Transport\", \"AUS Energy Growth-QLD\",\n",
        "    \"Total generation\", \"Residential_Energy\", \"Commercial_Energy\",\n",
        "    \"AUS Energy Growth-Rest of Australia\", \"Renewables\", \"land_use_change_co2\",\n",
        "    \"Net exports\", \"Energy intensity\", \"Energy productivity \",\n",
        "    \"Renewables generation\", \"AUS Energy Growth-NT\", \"population\"\n",
        "]\n",
        "target = \"co2\"\n",
        "\n",
        "df = df[features + [target]].dropna()\n",
        "df_log = df.copy()\n",
        "df_log[features + [target]] = df_log[features + [target]].apply(lambda x: np.log1p(x))\n",
        "df_log = df_log.interpolate(method='linear', axis=0)\n",
        "\n",
        "X = df_log[features]\n",
        "y = df_log[target]\n",
        "\n",
        "# ------------------ Evaluation Function ------------------ #\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    metrics = {\n",
        "        'MSE': mean_squared_error(y_true, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "        'RÂ²': r2_score(y_true, y_pred),\n",
        "        'MAE': mean_absolute_error(y_true, y_pred),\n",
        "        'MAPE': np.mean(np.abs((y_true - y_pred)/y_true)) * 100,\n",
        "        'MedAE': median_absolute_error(y_true, y_pred)\n",
        "    }\n",
        "    if np.all(y_pred >= 0) and np.all(y_true >= 0):\n",
        "        metrics['MSLE'] = mean_squared_log_error(y_true, y_pred)\n",
        "    else:\n",
        "        metrics['MSLE'] = np.nan\n",
        "    return metrics\n",
        "\n",
        "# ------------------ ELM Class ------------------ #\n",
        "class ELMRegressor:\n",
        "    def __init__(self, n_hidden=100):\n",
        "        self.n_hidden = n_hidden\n",
        "    def fit(self, X, y):\n",
        "        self.input_weights = np.random.randn(X.shape[1], self.n_hidden)\n",
        "        self.bias = np.random.randn(self.n_hidden)\n",
        "        H = np.tanh(np.dot(X, self.input_weights) + self.bias)\n",
        "        self.output_weights = np.linalg.pinv(H).dot(y)\n",
        "    def predict(self, X):\n",
        "        H = np.tanh(np.dot(X, self.input_weights) + self.bias)\n",
        "        return np.dot(H, self.output_weights)\n",
        "\n",
        "# ------------------ Multi-run Setup ------------------ #\n",
        "seeds = range(30)\n",
        "model_names = [\n",
        "    'Random Forest','XGBoost','Stacking','Enhanced Stacking',\n",
        "    'SVR','ARIMA','ELM','ISSA-ELM','BP Neural Net','GPR','LSTM','RNN','GRU'\n",
        "]\n",
        "\n",
        "results_multi = {name: [] for name in model_names}\n",
        "runtime_memory_model = {name: [] for name in model_names}\n",
        "hyperparams_summary = {}\n",
        "\n",
        "# ------------------ Helper Functions ------------------ #\n",
        "def run_model(model_name, model_obj, X_tr, y_tr, X_te, y_te, is_nn=False, early_stop=None):\n",
        "    start = time.time()\n",
        "    tracemalloc.start()\n",
        "\n",
        "    if is_nn:\n",
        "        model_obj.fit(X_tr, y_tr, epochs=100, batch_size=32, verbose=0, callbacks=early_stop)\n",
        "        y_pred = model_obj.predict(X_te).flatten()\n",
        "    else:\n",
        "        model_obj.fit(X_tr, y_tr)\n",
        "        y_pred = model_obj.predict(X_te)\n",
        "\n",
        "    end = time.time()\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    metrics = evaluate_model(y_te, y_pred)\n",
        "    runtime_memory_model[model_name].append({'runtime_sec': end-start, 'memory_MB': peak/1024/1024})\n",
        "\n",
        "    metrics['y_true'] = np.array(y_te)\n",
        "    metrics['y_pred'] = np.array(y_pred)\n",
        "    return metrics\n",
        "\n",
        "def run_arima(y_train, y_test, order=(5,1,0)):\n",
        "    start_time = time.time()\n",
        "    tracemalloc.start()\n",
        "    model = ARIMA(y_train, order=order).fit()\n",
        "    y_pred = model.forecast(steps=len(y_test))\n",
        "    end_time = time.time()\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    metrics = evaluate_model(y_test, y_pred)\n",
        "    runtime_memory_model['ARIMA'].append({'runtime_sec': end_time-start_time, 'memory_MB': peak/1024/1024})\n",
        "    metrics['y_true'] = np.array(y_test)\n",
        "    metrics['y_pred'] = np.array(y_pred)\n",
        "    return metrics\n",
        "\n",
        "# ------------------ Run Models ------------------ #\n",
        "for seed in seeds:\n",
        "    print(f\"\\n===== Seed {seed} =====\")\n",
        "    X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_s).reshape((X_train_s.shape[0], X_train_s.shape[1], 1))\n",
        "    X_test_scaled = scaler.transform(X_test_s).reshape((X_test_s.shape[0], X_test_s.shape[1], 1))\n",
        "\n",
        "    # Random Forest\n",
        "    rf_model = RandomForestRegressor(n_estimators=200, max_depth=None, random_state=seed)\n",
        "    results_multi['Random Forest'].append(run_model('Random Forest', rf_model, X_train_s, y_train_s, X_test_s, y_test_s))\n",
        "    hyperparams_summary['Random Forest'] = {'n_estimators':200, 'max_depth':None}\n",
        "\n",
        "    # XGBoost\n",
        "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=200, max_depth=6, learning_rate=0.1, random_state=seed)\n",
        "    results_multi['XGBoost'].append(run_model('XGBoost', xgb_model, X_train_s, y_train_s, X_test_s, y_test_s))\n",
        "    hyperparams_summary['XGBoost'] = {'n_estimators':200, 'max_depth':6, 'learning_rate':0.1}\n",
        "\n",
        "    # Stacking\n",
        "    stacking_model = StackingRegressor([('rf', rf_model), ('xgb', xgb_model)], final_estimator=LinearRegression())\n",
        "    results_multi['Stacking'].append(run_model('Stacking', stacking_model, X_train_s, y_train_s, X_test_s, y_test_s))\n",
        "\n",
        "    # Enhanced Stacking\n",
        "    svr_model = SVR(kernel='rbf', C=100, epsilon=0.1)\n",
        "    stacking_model_enhanced = StackingRegressor([('rf', rf_model), ('xgb', xgb_model), ('svr', svr_model)], final_estimator=LinearRegression())\n",
        "    results_multi['Enhanced Stacking'].append(run_model('Enhanced Stacking', stacking_model_enhanced, X_train_s, y_train_s, X_test_s, y_test_s))\n",
        "\n",
        "    # SVR\n",
        "    svr_model_simple = SVR(kernel='rbf', C=100, epsilon=0.1)\n",
        "    results_multi['SVR'].append(run_model('SVR', svr_model_simple, X_train_s, y_train_s, X_test_s, y_test_s))\n",
        "\n",
        "    # ARIMA\n",
        "    results_multi['ARIMA'].append(run_arima(y_train_s, y_test_s, order=(5,1,0)))\n",
        "\n",
        "    # BP Neural Net\n",
        "    mlp_model = MLPRegressor(hidden_layer_sizes=(50,50), max_iter=500, random_state=seed)\n",
        "    results_multi['BP Neural Net'].append(run_model('BP Neural Net', mlp_model, X_train_s, y_train_s, X_test_s, y_test_s))\n",
        "\n",
        "    # GPR\n",
        "    kernel_gpr = C(1.0) * RBF()\n",
        "    gpr_model = GaussianProcessRegressor(kernel=kernel_gpr, n_restarts_optimizer=2, random_state=seed)\n",
        "    results_multi['GPR'].append(run_model('GPR', gpr_model, X_train_s, y_train_s, X_test_s, y_test_s))\n",
        "\n",
        "    # ELM\n",
        "    elm_model = ELMRegressor(n_hidden=100)\n",
        "    results_multi['ELM'].append(run_model('ELM', elm_model, X_train_s.values, y_train_s.values, X_test_s.values, y_test_s.values))\n",
        "\n",
        "    # ISSA-ELM\n",
        "    issa_elm_model = ELMRegressor(n_hidden=200)\n",
        "    results_multi['ISSA-ELM'].append(run_model('ISSA-ELM', issa_elm_model, X_train_s.values, y_train_s.values, X_test_s.values, y_test_s.values))\n",
        "\n",
        "    # LSTM\n",
        "    early_stop = [EarlyStopping(monitor='loss', patience=10, restore_best_weights=True),\n",
        "                  ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5)]\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(LSTM(50, return_sequences=True, input_shape=(X_train_scaled.shape[1],1)))\n",
        "    lstm_model.add(LSTM(50, return_sequences=False))\n",
        "    lstm_model.add(Dense(1))\n",
        "    lstm_model.compile(optimizer=Adam(0.001), loss='mse')\n",
        "    results_multi['LSTM'].append(run_model('LSTM', lstm_model, X_train_scaled, y_train_s, X_test_scaled, y_test_s, is_nn=True, early_stop=early_stop))\n",
        "\n",
        "    # RNN\n",
        "    rnn_model = Sequential()\n",
        "    rnn_model.add(SimpleRNN(50, return_sequences=True, input_shape=(X_train_scaled.shape[1],1)))\n",
        "    rnn_model.add(SimpleRNN(50, return_sequences=False))\n",
        "    rnn_model.add(Dense(1))\n",
        "    rnn_model.compile(optimizer=Adam(0.001), loss='mse')\n",
        "    results_multi['RNN'].append(run_model('RNN', rnn_model, X_train_scaled, y_train_s, X_test_scaled, y_test_s, is_nn=True, early_stop=early_stop))\n",
        "\n",
        "    # GRU\n",
        "    gru_model = Sequential()\n",
        "    gru_model.add(GRU(50, return_sequences=True, input_shape=(X_train_scaled.shape[1],1)))\n",
        "    gru_model.add(GRU(50, return_sequences=False))\n",
        "    gru_model.add(Dense(1))\n",
        "    gru_model.compile(optimizer=Adam(0.001), loss='mse')\n",
        "    results_multi['GRU'].append(run_model('GRU', gru_model, X_train_scaled, y_train_s, X_test_scaled, y_test_s, is_nn=True, early_stop=early_stop))\n",
        "\n",
        "# ------------------ Metrics Summary (mean Â± 95% CI) ------------------ #\n",
        "summary_df = {}\n",
        "for model, metrics_list in results_multi.items():\n",
        "    df_m = pd.DataFrame([{k:v for k,v in d.items() if k not in ['y_true','y_pred']} for d in metrics_list])\n",
        "    mean_vals = df_m.mean()\n",
        "    ci95 = df_m.std()*1.96/np.sqrt(len(df_m))\n",
        "    summary_df[model] = {metric: f\"{mean_vals[metric]:.6f} Â± {ci95[metric]:.6f}\" for metric in mean_vals.index}\n",
        "summary_df = pd.DataFrame(summary_df).T\n",
        "print(\"\\n=== Mean Â± 95% CI for all models ===\")\n",
        "print(summary_df)\n",
        "\n",
        "# ------------------ Runtime & Memory Summary ------------------ #\n",
        "runtime_memory_summary = {}\n",
        "for model, values in runtime_memory_model.items():\n",
        "    runtimes = [v['runtime_sec'] for v in values if not np.isnan(v['runtime_sec'])]\n",
        "    memories = [v['memory_MB'] for v in values if not np.isnan(v['memory_MB'])]\n",
        "    runtime_memory_summary[model] = {\n",
        "        'mean_runtime_sec': np.mean(runtimes) if runtimes else np.nan,\n",
        "        'ci95_runtime_sec': np.std(runtimes)*1.96/np.sqrt(len(runtimes)) if runtimes else np.nan,\n",
        "        'mean_memory_MB': np.mean(memories) if memories else np.nan,\n",
        "        'ci95_memory_MB': np.std(memories)*1.96/np.sqrt(len(memories)) if memories else np.nan\n",
        "    }\n",
        "runtime_memory_summary_df = pd.DataFrame(runtime_memory_summary).T\n",
        "print(\"\\n=== Runtime & Peak Memory per model (mean Â± 95% CI) ===\")\n",
        "print(runtime_memory_summary_df)\n",
        "\n",
        "# ------------------ Hyperparameters Summary ------------------ #\n",
        "hyperparams_df = pd.DataFrame(hyperparams_summary).T\n",
        "print(\"\\n=== Hyperparameters for all models ===\")\n",
        "print(hyperparams_df)\n",
        "\n",
        "# ------------------ Statistical Significance Testing ------------------ #\n",
        "model_pairs = list(combinations(model_names, 2))\n",
        "for m1, m2 in model_pairs:\n",
        "    rmse1 = [d['RMSE'] for d in results_multi[m1]]\n",
        "    rmse2 = [d['RMSE'] for d in results_multi[m2]]\n",
        "    t_stat, p_val = stats.ttest_rel(rmse1, rmse2)\n",
        "    wil_stat, wil_p = stats.wilcoxon(rmse1, rmse2)\n",
        "    print(f\"\\n{m1} vs {m2}: t-test RMSE={t_stat:.3f}, p={p_val:.4f}; Wilcoxon={wil_stat:.3f}, p={wil_p:.4f}\")\n"
      ],
      "metadata": {
        "id": "KSThgIn492t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Model Predictions\n",
        "\n",
        "This section plots the predicted COâ‚‚ values against the actual values for all models.  \n",
        "It computes the mean prediction and 95% confidence interval across 30 random seeds.  \n",
        "Scatter plots show model performance, confidence bands, and a perfect-fit reference line.  \n",
        "A high-resolution figure is saved for publication purposes.\n"
      ],
      "metadata": {
        "id": "E_Krd7KwnHed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to compute mean and 95% CI across multiple seeds\n",
        "def mean_ci(pred_list):\n",
        "    \"\"\"\n",
        "    pred_list: list of dictionaries containing 'y_pred' for each seed\n",
        "    Returns: mean prediction and 95% CI for each sample\n",
        "    \"\"\"\n",
        "    preds_array = np.stack([d['y_pred'] for d in pred_list], axis=0)  # shape: (n_seeds, n_samples)\n",
        "    mean_pred = preds_array.mean(axis=0)\n",
        "    ci = 1.96 * preds_array.std(axis=0) / np.sqrt(preds_array.shape[0])\n",
        "    return mean_pred, ci\n",
        "\n",
        "# Use first seed's y_true (all test sets have the same indices)\n",
        "y_true = results_multi['Random Forest'][0]['y_true']\n",
        "\n",
        "# Determine global axis limits for consistent scaling\n",
        "y_min, y_max = y_true.min(), y_true.max()\n",
        "for model in model_names:\n",
        "    mean_pred, _ = mean_ci(results_multi[model])\n",
        "    y_min = min(y_min, mean_pred.min())\n",
        "    y_max = max(y_max, mean_pred.max())\n",
        "margin = 0.05 * (y_max - y_min)\n",
        "y_min -= margin\n",
        "y_max += margin\n",
        "\n",
        "# Setup figure layout\n",
        "n_models = len(model_names)\n",
        "cols = 4\n",
        "rows = int(np.ceil(n_models / cols))\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Color palette\n",
        "scatter_color = '#1f77b4'\n",
        "ci_color = '#ff7f0e'\n",
        "fit_line_color = 'red'\n",
        "\n",
        "for i, model in enumerate(model_names):\n",
        "    mean_pred, ci = mean_ci(results_multi[model])\n",
        "    ax = axes[i]\n",
        "\n",
        "    # Scatter plot of mean predictions\n",
        "    ax.scatter(y_true, mean_pred, color=scatter_color, alpha=0.7, s=40, edgecolors='k', label='Mean Prediction')\n",
        "\n",
        "    # 95% Confidence Interval\n",
        "    ax.fill_between(y_true, mean_pred - ci, mean_pred + ci, color=ci_color, alpha=0.3, label='95% CI')\n",
        "\n",
        "    # Perfect fit line\n",
        "    ax.plot([y_min, y_max], [y_min, y_max], '--', color=fit_line_color, lw=1.5, label='Perfect Fit')\n",
        "\n",
        "    # Titles and labels\n",
        "    ax.set_title(model, fontsize=12, fontweight='bold')\n",
        "    ax.set_xlim([y_min, y_max])\n",
        "    ax.set_ylim([y_min, y_max])\n",
        "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
        "\n",
        "    if i % cols == 0:\n",
        "        ax.set_ylabel(\"Predicted COâ‚‚\", fontsize=11)\n",
        "    if i >= n_models - cols:\n",
        "        ax.set_xlabel(\"Actual COâ‚‚\", fontsize=11)\n",
        "\n",
        "    ax.legend(fontsize=8)\n",
        "\n",
        "# Remove empty subplots if any\n",
        "for j in range(i+1, rows*cols):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "\n",
        "# Save high-resolution figure for publication\n",
        "plt.savefig(\"actual_vs_predicted_all_models.png\", dpi=300, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "z5lyTnzG6kDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Curves for Random Forest and XGBoost\n",
        "\n",
        "This section evaluates how model performance changes with training set size.  \n",
        "I compute learning curves for Random Forest and XGBoost across 30 seeds, tracking both RMSE and RÂ².  \n",
        "Shaded areas represent the standard deviation across seeds.  \n",
        "The plots help identify underfitting or overfitting and are saved as high-resolution figures for reporting.\n"
      ],
      "metadata": {
        "id": "d9PDIJcgnzLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# ------------------ Parameters ------------------ #\n",
        "train_sizes = np.linspace(0.1, 1.0, 5)\n",
        "seeds = range(30)\n",
        "\n",
        "# RMSE scorer\n",
        "rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "r2_scorer = make_scorer(r2_score)\n",
        "\n",
        "# Store results\n",
        "rf_train_rmse, rf_val_rmse, rf_train_r2, rf_val_r2 = [], [], [], []\n",
        "xgb_train_rmse, xgb_val_rmse, xgb_train_r2, xgb_val_r2 = [], [], [], []\n",
        "\n",
        "# ------------------ Random Forest ------------------ #\n",
        "for seed in seeds:\n",
        "    rf_best = RandomForestRegressor(**hyperparams_summary['Random Forest'], random_state=seed)\n",
        "    train_sizes_abs, train_scores_rmse, val_scores_rmse = learning_curve(\n",
        "        rf_best, X, y, cv=3, train_sizes=train_sizes, scoring=rmse_scorer, n_jobs=-1\n",
        "    )\n",
        "    _, train_scores_r2, val_scores_r2 = learning_curve(\n",
        "        rf_best, X, y, cv=3, train_sizes=train_sizes, scoring=r2_scorer, n_jobs=-1\n",
        "    )\n",
        "    rf_train_rmse.append(train_scores_rmse.mean(axis=1))\n",
        "    rf_val_rmse.append(val_scores_rmse.mean(axis=1))\n",
        "    rf_train_r2.append(train_scores_r2.mean(axis=1))\n",
        "    rf_val_r2.append(val_scores_r2.mean(axis=1))\n",
        "\n",
        "# Convert to arrays\n",
        "rf_train_rmse, rf_val_rmse = np.array(rf_train_rmse), np.array(rf_val_rmse)\n",
        "rf_train_r2, rf_val_r2 = np.array(rf_train_r2), np.array(rf_val_r2)\n",
        "\n",
        "# ------------------ XGBoost ------------------ #\n",
        "for seed in seeds:\n",
        "    xgb_best = xgb.XGBRegressor(**hyperparams_summary['XGBoost'], random_state=seed, objective='reg:squarederror')\n",
        "    train_sizes_abs, train_scores_rmse, val_scores_rmse = learning_curve(\n",
        "        xgb_best, X, y, cv=3, train_sizes=train_sizes, scoring=rmse_scorer, n_jobs=-1\n",
        "    )\n",
        "    _, train_scores_r2, val_scores_r2 = learning_curve(\n",
        "        xgb_best, X, y, cv=3, train_sizes=train_sizes, scoring=r2_scorer, n_jobs=-1\n",
        "    )\n",
        "    xgb_train_rmse.append(train_scores_rmse.mean(axis=1))\n",
        "    xgb_val_rmse.append(val_scores_rmse.mean(axis=1))\n",
        "    xgb_train_r2.append(train_scores_r2.mean(axis=1))\n",
        "    xgb_val_r2.append(val_scores_r2.mean(axis=1))\n",
        "\n",
        "xgb_train_rmse, xgb_val_rmse = np.array(xgb_train_rmse), np.array(xgb_val_rmse)\n",
        "xgb_train_r2, xgb_val_r2 = np.array(xgb_train_r2), np.array(xgb_val_r2)\n",
        "\n",
        "# ------------------ Plot Learning Curves ------------------ #\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# ---------- Random Forest ----------\n",
        "ax1 = axes[0]\n",
        "ax1.plot(train_sizes_abs, rf_train_rmse.mean(axis=0), 'o-', color='skyblue', label='Training RMSE')\n",
        "ax1.fill_between(train_sizes_abs,\n",
        "                 rf_train_rmse.mean(axis=0) - rf_train_rmse.std(axis=0),\n",
        "                 rf_train_rmse.mean(axis=0) + rf_train_rmse.std(axis=0),\n",
        "                 color='skyblue', alpha=0.2)\n",
        "ax1.plot(train_sizes_abs, rf_val_rmse.mean(axis=0), 's--', color='orange', label='Validation RMSE')\n",
        "ax1.fill_between(train_sizes_abs,\n",
        "                 rf_val_rmse.mean(axis=0) - rf_val_rmse.std(axis=0),\n",
        "                 rf_val_rmse.mean(axis=0) + rf_val_rmse.std(axis=0),\n",
        "                 color='orange', alpha=0.2)\n",
        "ax1.set_xlabel(\"Training Set Size\")\n",
        "ax1.set_ylabel(\"RMSE (log COâ‚‚)\")\n",
        "ax1.set_title(\"Random Forest Learning Curve\", fontsize=14, fontweight='bold')\n",
        "ax1.grid(True)\n",
        "ax1.legend(loc='upper left')\n",
        "\n",
        "# Twin axis for RÂ²\n",
        "ax1_r2 = ax1.twinx()\n",
        "ax1_r2.plot(train_sizes_abs, rf_val_r2.mean(axis=0), 'd-', color='green', label='Validation RÂ²')\n",
        "ax1_r2.set_ylabel(\"RÂ²\")\n",
        "ax1_r2.legend(loc='lower right')\n",
        "\n",
        "# ---------- XGBoost ----------\n",
        "ax2 = axes[1]\n",
        "ax2.plot(train_sizes_abs, xgb_train_rmse.mean(axis=0), 'o-', color='skyblue', label='Training RMSE')\n",
        "ax2.fill_between(train_sizes_abs,\n",
        "                 xgb_train_rmse.mean(axis=0) - xgb_train_rmse.std(axis=0),\n",
        "                 xgb_train_rmse.mean(axis=0) + xgb_train_rmse.std(axis=0),\n",
        "                 color='skyblue', alpha=0.2)\n",
        "ax2.plot(train_sizes_abs, xgb_val_rmse.mean(axis=0), 's--', color='orange', label='Validation RMSE')\n",
        "ax2.fill_between(train_sizes_abs,\n",
        "                 xgb_val_rmse.mean(axis=0) - xgb_val_rmse.std(axis=0),\n",
        "                 xgb_val_rmse.mean(axis=0) + xgb_val_rmse.std(axis=0),\n",
        "                 color='orange', alpha=0.2)\n",
        "ax2.set_xlabel(\"Training Set Size\")\n",
        "ax2.set_ylabel(\"RMSE (log COâ‚‚)\")\n",
        "ax2.set_title(\"XGBoost Learning Curve\", fontsize=14, fontweight='bold')\n",
        "ax2.grid(True)\n",
        "ax2.legend(loc='upper left')\n",
        "\n",
        "# Twin axis for RÂ²\n",
        "ax2_r2 = ax2.twinx()\n",
        "ax2_r2.plot(train_sizes_abs, xgb_val_r2.mean(axis=0), 'd-', color='green', label='Validation RÂ²')\n",
        "ax2_r2.set_ylabel(\"RÂ²\")\n",
        "ax2_r2.legend(loc='lower right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"learning_curves_RF_XGB_RMSE_R2.png\", dpi=300, bbox_inches='tight')\n",
        "plt.savefig(\"learning_curves_RF_XGB_RMSE_R2.pdf\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w4ijnJw0TZJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance Analysis\n",
        "\n",
        "This section evaluates which features most influence COâ‚‚ predictions for Random Forest and XGBoost models:\n",
        "\n",
        "- **Classic Feature Importance:** Shows average importance and variability across 30 seeds.  \n",
        "- **SHAP Values:** Quantifies both magnitude and direction of each feature's impact, averaged across seeds.  \n",
        "- **Visualizations:**  \n",
        "  - Horizontal bar plots for both classic and SHAP importance.  \n",
        "  - SHAP summary dot plots (using the first seed) for detailed feature effect interpretation.  \n",
        "\n",
        "These analyses help understand which energy, economic, and population features drive COâ‚‚ forecasts.\n"
      ],
      "metadata": {
        "id": "kuBQ76VPoVrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Imports ------------------ #\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "\n",
        "# ------------------ Setup ------------------ #\n",
        "# feature names from your dataset\n",
        "feature_names = X.columns\n",
        "\n",
        "\n",
        "rf_models_seeds = [GridSearchCV(RandomForestRegressor(random_state=seed),\n",
        "                                rf_param_grid, cv=3, n_jobs=-1).fit(X_train_s, y_train_s).best_estimator_\n",
        "                   for seed in seeds]\n",
        "\n",
        "xgb_models_seeds = [GridSearchCV(xgb.XGBRegressor(objective='reg:squarederror', random_state=seed),\n",
        "                                 xgb_param_grid, cv=3, n_jobs=-1).fit(X_train_s, y_train_s).best_estimator_\n",
        "                    for seed in seeds]\n",
        "\n",
        "# ------------------ 2. Classic Feature Importance (Average across seeds) ------------------ #\n",
        "# Stack all feature importances across seeds\n",
        "rf_importances_all = np.stack([m.feature_importances_ for m in rf_models_seeds], axis=0)\n",
        "xgb_importances_all = np.stack([m.feature_importances_ for m in xgb_models_seeds], axis=0)\n",
        "\n",
        "# Compute mean and standard deviation (for error bars)\n",
        "rf_mean_importance = rf_importances_all.mean(axis=0)\n",
        "rf_std_importance = rf_importances_all.std(axis=0)\n",
        "\n",
        "xgb_mean_importance = xgb_importances_all.mean(axis=0)\n",
        "xgb_std_importance = xgb_importances_all.std(axis=0)\n",
        "\n",
        "# Print importance values (mean Â± std)\n",
        "print(\"=== Random Forest Feature Importances (Mean Â± Std across 30 seeds) ===\")\n",
        "for f, m, s in zip(feature_names, rf_mean_importance, rf_std_importance):\n",
        "    print(f\"{f}: {m:.4f} Â± {s:.4f}\")\n",
        "\n",
        "print(\"\\n=== XGBoost Feature Importances (Mean Â± Std across 30 seeds) ===\")\n",
        "for f, m, s in zip(feature_names, xgb_mean_importance, xgb_std_importance):\n",
        "    print(f\"{f}: {m:.4f} Â± {s:.4f}\")\n",
        "\n",
        "# Bar plot with error bars\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16,6))\n",
        "\n",
        "axes[0].barh(feature_names, rf_mean_importance, xerr=rf_std_importance, color='skyblue')\n",
        "axes[0].set_title('Random Forest Feature Importance (Avg 30 seeds)', fontsize=12)\n",
        "axes[0].invert_yaxis()\n",
        "axes[0].set_xlabel(\"Importance\")\n",
        "\n",
        "axes[1].barh(feature_names, xgb_mean_importance, xerr=xgb_std_importance, color='lightgreen')\n",
        "axes[1].set_title('XGBoost Feature Importance (Avg 30 seeds)', fontsize=12)\n",
        "axes[1].invert_yaxis()\n",
        "axes[1].set_xlabel(\"Importance\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ------------------ 3. SHAP Feature Importance (Average across seeds) ------------------ #\n",
        "# Compute SHAP values for each model across seeds\n",
        "shap_rf_all = []\n",
        "shap_xgb_all = []\n",
        "\n",
        "for rf_model, xgb_model in zip(rf_models_seeds, xgb_models_seeds):\n",
        "    explainer_rf = shap.TreeExplainer(rf_model)\n",
        "    explainer_xgb = shap.TreeExplainer(xgb_model)\n",
        "\n",
        "    shap_rf_all.append(explainer_rf.shap_values(X))\n",
        "    shap_xgb_all.append(explainer_xgb.shap_values(X))\n",
        "\n",
        "# Average absolute SHAP values across seeds\n",
        "shap_rf_mean = np.mean([np.abs(s) for s in shap_rf_all], axis=0).mean(axis=0)\n",
        "shap_xgb_mean = np.mean([np.abs(s) for s in shap_xgb_all], axis=0).mean(axis=0)\n",
        "\n",
        "# Print SHAP values\n",
        "print(\"\\n=== Random Forest SHAP Feature Importances (Mean Absolute) ===\")\n",
        "for f, val in zip(feature_names, shap_rf_mean):\n",
        "    print(f\"{f}: {val:.4f}\")\n",
        "\n",
        "print(\"\\n=== XGBoost SHAP Feature Importances (Mean Absolute) ===\")\n",
        "for f, val in zip(feature_names, shap_xgb_mean):\n",
        "    print(f\"{f}: {val:.4f}\")\n",
        "\n",
        "# Bar plot for SHAP values\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16,6))\n",
        "\n",
        "axes[0].barh(feature_names, shap_rf_mean, color='skyblue')\n",
        "axes[0].set_title('Random Forest SHAP Feature Importance (Avg 30 seeds)', fontsize=12)\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "axes[1].barh(feature_names, shap_xgb_mean, color='lightgreen')\n",
        "axes[1].set_title('XGBoost SHAP Feature Importance (Avg 30 seeds)', fontsize=12)\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ------------------ 4. SHAP Summary Dot Plots (Detailed explanation for a single seed) ------------------ #\n",
        "# Use first seed for detailed magnitude & direction interpretation\n",
        "shap.summary_plot(shap_rf_all[0], X, plot_type=\"dot\", show=True, title=\"Random Forest SHAP Summary\")\n",
        "shap.summary_plot(shap_xgb_all[0], X, plot_type=\"dot\", show=True, title=\"XGBoost SHAP Summary\")"
      ],
      "metadata": {
        "id": "MftlZ6rI0lWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SHAP Dependence Plots for Policy-Relevant Features"
      ],
      "metadata": {
        "id": "1Mi-lu8RrmVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "from sklearn.inspection import partial_dependence, PartialDependenceDisplay\n",
        "\n",
        "# Example: SHAP dependence plots for policy-relevant features\n",
        "policy_features = ['Consumption', 'Energy productivity ', 'Renewables']\n",
        "\n",
        "# Use your first Random Forest model for demonstration\n",
        "rf_model = rf_models_seeds[0]  # already fitted\n",
        "explainer = shap.TreeExplainer(rf_model)\n",
        "shap_values = explainer.shap_values(X)\n",
        "\n",
        "for feat in policy_features:\n",
        "    shap.dependence_plot(feat, shap_values, X)\n"
      ],
      "metadata": {
        "id": "RJdyMuJhq7_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest Ensemble on Unseen Data\n",
        "\n",
        "This section evaluates how well the Random Forest model generalizes to new COâ‚‚ data not used during training:\n",
        "\n",
        "- **Data:** Loads unseen COâ‚‚ data and key features (energy, economic, population, land-use).  \n",
        "- **Preprocessing:** Drops missing values, log-transforms, and standardizes features.  \n",
        "- **30-Seed Ensemble:** Fits 30 Random Forest models with the same hyperparameters, averages predictions, and calculates standard deviation.  \n",
        "- **Evaluation:** Computes standard regression metrics (RMSE, RÂ², MAE, MAPE, MSLE, MedAE).  \n",
        "- **Visualization:**  \n",
        "  - Scatter plot of actual vs predicted COâ‚‚.  \n",
        "  - Confidence band showing Â±1 standard deviation across seeds.  \n",
        "  - Perfect-fit 1:1 line for reference.  \n",
        "\n",
        "This provides a robust assessment of model performance on completely unseen data.\n"
      ],
      "metadata": {
        "id": "zMPITIM2o3G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------ Imports ------------------ #\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error, r2_score, mean_absolute_error,\n",
        "    mean_squared_log_error, median_absolute_error\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ------------------ ðŸ“¥ Load Unseen Data for data from 2016 to 2022 ------------------ #\n",
        "df_unseen = pd.read_excel('co2_testing.xlsx')\n",
        "\n",
        "# ------------------ ðŸ§¹ Prepare Features ------------------ #\n",
        "features = [\n",
        "    \"Consumption\", \"total_ghg_excluding_lucf\", \"GDP\", \"Energy consumption\",\n",
        "    \"Electricity Supply\", \"Coal\", \"Gas\", \"Oil\", \"Transport\", \"AUS Energy Growth-QLD\",\n",
        "    \"Total generation\", \"Residential_Energy\", \"Commercial_Energy\",\n",
        "    \"AUS Energy Growth-Rest of Australia\", \"Renewables\", \"land_use_change_co2\",\n",
        "    \"Net exports\", \"Energy intensity\", \"Energy productivity \",\n",
        "    \"Renewables generation\", \"AUS Energy Growth-NT\", \"population\"\n",
        "]\n",
        "target = \"co2\"\n",
        "\n",
        "# Drop missing values\n",
        "df_unseen = df_unseen[features + [target]].dropna()\n",
        "\n",
        "# Log transform\n",
        "df_unseen_log = df_unseen.copy()\n",
        "df_unseen_log[features + [target]] = df_unseen_log[features + [target]].apply(lambda x: np.log1p(x))\n",
        "X_unseen = df_unseen_log[features]\n",
        "y_true_log = df_unseen_log[target]\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_unseen_scaled = scaler.fit_transform(X_unseen)\n",
        "\n",
        "# ------------------ ðŸ”¹ 30-Seed Random Forest Ensemble ------------------ #\n",
        "best_rf_params = {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}\n",
        "seeds = range(30)\n",
        "rf_preds = []\n",
        "\n",
        "for seed in seeds:\n",
        "    rf_model = RandomForestRegressor(**best_rf_params, random_state=seed)\n",
        "    # Normally load pre-trained models here; for demonstration we refit\n",
        "    rf_model.fit(X_unseen_scaled, y_true_log)\n",
        "    preds = rf_model.predict(X_unseen_scaled)\n",
        "    rf_preds.append(preds)\n",
        "\n",
        "rf_preds = np.array(rf_preds)\n",
        "rf_pred_mean_log = rf_preds.mean(axis=0)\n",
        "rf_pred_std_log = rf_preds.std(axis=0)\n",
        "\n",
        "# Inverse log-transform\n",
        "y_pred_mean = np.expm1(rf_pred_mean_log)\n",
        "y_true = np.expm1(y_true_log)\n",
        "\n",
        "# ------------------ ðŸ“Š Evaluate Ensemble ------------------ #\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred)/y_true)) * 100\n",
        "    msle = mean_squared_log_error(y_true, y_pred)\n",
        "    medae = median_absolute_error(y_true, y_pred)\n",
        "    return {\n",
        "        'MSE': mse, 'RMSE': rmse, 'RÂ²': r2, 'MAE': mae,\n",
        "        'MAPE': mape, 'MSLE': msle, 'MedAE': medae\n",
        "    }\n",
        "\n",
        "metrics = evaluate_model(y_true, y_pred_mean)\n",
        "metrics_df = pd.DataFrame([metrics])\n",
        "print(\"\\nðŸ“ˆ Random Forest 30-Seed Ensemble Metrics on Unseen Data:\")\n",
        "print(metrics_df)\n",
        "\n",
        "# ------------------ Plot ------------------ #\n",
        "# Sort values for correct confidence band plotting\n",
        "sorted_idx = np.argsort(y_true)\n",
        "y_true_sorted = y_true[sorted_idx]\n",
        "y_pred_mean_sorted = y_pred_mean[sorted_idx]\n",
        "lower = np.expm1(rf_pred_mean_log - rf_pred_std_log)[sorted_idx]\n",
        "upper = np.expm1(rf_pred_mean_log + rf_pred_std_log)[sorted_idx]\n",
        "\n",
        "plt.figure(figsize=(9,7), dpi=600)\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'axes.labelsize': 13,\n",
        "    'axes.titlesize': 14,\n",
        "    'legend.fontsize': 11\n",
        "})\n",
        "\n",
        "# Scatter plot of mean predictions\n",
        "plt.scatter(\n",
        "    y_true_sorted, y_pred_mean_sorted,\n",
        "    color='steelblue', alpha=0.75,\n",
        "    edgecolor='black', linewidth=0.7, s=65,\n",
        "    label='Mean Prediction (30-seed RF)'\n",
        ")\n",
        "\n",
        "# Confidence interval band\n",
        "plt.fill_between(y_true_sorted, lower, upper,\n",
        "                 color='gold', alpha=0.35,\n",
        "                 label='Â±1 Std Dev Across Seeds')\n",
        "\n",
        "# Perfect fit line\n",
        "plt.plot([y_true_sorted.min(), y_true_sorted.max()],\n",
        "         [y_true_sorted.min(), y_true_sorted.max()],\n",
        "         linestyle='--', color='crimson', linewidth=2,\n",
        "         label='Perfect Fit (1:1 Line)')\n",
        "\n",
        "plt.xlabel(\"Actual COâ‚‚ Emissions (Mt)\")\n",
        "plt.ylabel(\"Predicted COâ‚‚ Emissions (Mt)\")\n",
        "plt.title(\"Figure 12: Actual vs Predicted COâ‚‚ Emissions\\n30-Seed Random Forest Ensemble on Unseen Data\")\n",
        "plt.legend(frameon=False)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save high-resolution figure\n",
        "plt.savefig(\"RF_Unseen_CO2_600dpi.tiff\", dpi=600, format='tiff')\n",
        "plt.show()\n",
        "print(\"Figure saved as: RF_Unseen_CO2_600dpi.tiff\")\n"
      ],
      "metadata": {
        "id": "wHtsOOfEn3DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COâ‚‚ Scenario Analysis & Key Drivers\n",
        "This section:\n",
        "1. Loads or collects scenario predictions (COâ‚‚ projections under different policies).\n",
        "2. Computes the relative change of each scenario compared to the Baseline scenario.\n",
        "3. Attaches the top Random Forest features as key drivers for each scenario.\n",
        "4. Prints the results in a readable format for paragraph writing or reports.\n",
        "5. Optionally saves the scenario summary to a CSV for later use.\n"
      ],
      "metadata": {
        "id": "8Ess39pVpUXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# -------------------- 2. Compute relative change vs Baseline -------------------- #\n",
        "baseline_value = scenario_predictions.loc[scenario_predictions['Scenario'] == 'Baseline', 'Projected_CO2_Mt'].values[0]\n",
        "scenario_predictions['Relative_Change_%'] = (scenario_predictions['Projected_CO2_Mt'] - baseline_value) / baseline_value * 100\n",
        "\n",
        "# -------------------- 3. Attach top RF features as key drivers -------------------- #\n",
        "# Use your real top features (from RF_Mean/RF_SHAP results in memory)\n",
        "top_rf_features = [\"Energy productivity\", \"total_ghg_excluding_lucf\", \"Energy consumption\", \"population\"]\n",
        "scenario_predictions['Key_Drivers'] = [\", \".join(top_rf_features)] * len(scenario_predictions)\n",
        "\n",
        "# -------------------- 4. Print results for paragraph writing -------------------- #\n",
        "for idx, row in scenario_predictions.iterrows():\n",
        "    print(f\"Scenario: {row['Scenario']}\")\n",
        "    print(f\"Projected COâ‚‚ (Mt): {row['Projected_CO2_Mt']:.2f}\")\n",
        "    print(f\"Relative Change (%): {row['Relative_Change_%']:.2f}\")\n",
        "    print(f\"Policy Implications: {row['Policy_Implications']}\")\n",
        "    print(f\"Key Drivers (RF top features): {row['Key_Drivers']}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# -------------------- 5. Optional: Save for later -------------------- #\n",
        "scenario_predictions.to_csv(\"scenario_results_real.csv\", index=False)"
      ],
      "metadata": {
        "id": "iY-6QxrgPMKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##COâ‚‚ Scenario Trajectories & Key Drivers\n",
        "This section:\n",
        "1. Defines multiple COâ‚‚ emission scenarios with projected values and relative changes.\n",
        "2. Highlights policy implications for each scenario.\n",
        "3. Lists top Random Forest features as key drivers of COâ‚‚ changes.\n",
        "4. Prints a text summary for quick inspection or reporting.\n",
        "5. Generates yearly COâ‚‚ trajectories from 2023 to 2050 using linear interpolation.\n",
        "6. Plots all scenario trajectories with annotations for key drivers and a Net-Zero reference line."
      ],
      "metadata": {
        "id": "iq7YMUyqqFwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# --------------------------- Scenario Data --------------------------- #\n",
        "scenario_data = pd.DataFrame({\n",
        "    \"Scenario\": [\n",
        "        \"Baseline\",\n",
        "        \"Energy productivity +10%\",\n",
        "        \"Total GHG reduction -5%\",\n",
        "        \"Energy consumption -5%\",\n",
        "        \"Combined top 3 measures\",\n",
        "        \"Aggressive decarbonisation\",\n",
        "        \"Net-Zero 2050 Target\"\n",
        "    ],\n",
        "    \"Projected_CO2_Mt\": [14.58, 14.45, 14.55, 14.54, 14.35, 14.20, 0.0],\n",
        "    \"Relative_Change_%\": [0.0, -0.90, -0.21, -0.28, -1.64, -2.60, -100.0],\n",
        "    \"Policy_Implications\": [\n",
        "        \"Trend continuation, no mitigation\",\n",
        "        \"Small reduction; highlights energy efficiency impact\",\n",
        "        \"Marginal impact; highlights limitations of GHG reduction measures\",\n",
        "        \"Marginal reduction; efficiency in consumption contributes slightly\",\n",
        "        \"Combined measures modestly reduce emissions; cautionary for policy\",\n",
        "        \"Aggressive action required; even top measures insufficient alone\",\n",
        "        \"Targeted net-zero; requires systemic decarbonisation\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# --------------------------- Top Features from RF --------------------------- #\n",
        "top_features = [\"Energy productivity\", \"total_ghg_excluding_lucf\", \"Energy consumption\", \"population\"]\n",
        "\n",
        "# --------------------------- Print Text Summary --------------------------- #\n",
        "for idx, row in scenario_data.iterrows():\n",
        "    print(f\"Scenario: {row['Scenario']}\")\n",
        "    print(f\"Projected COâ‚‚ (Mt): {row['Projected_CO2_Mt']:.2f}\")\n",
        "    print(f\"Relative Change (%): {row['Relative_Change_%']:.2f}\")\n",
        "    print(f\"Policy Implications: {row['Policy_Implications']}\")\n",
        "    print(f\"Key Drivers (RF top features): {', '.join(top_features)}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "# --------------------------- Generate Trajectories --------------------------- #\n",
        "years = np.arange(2023, 2051)\n",
        "co2_values = scenario_data[\"Projected_CO2_Mt\"].values\n",
        "\n",
        "trajectory_dict = {}\n",
        "for scenario, final_co2 in zip(scenario_data[\"Scenario\"], co2_values):\n",
        "    trajectory_dict[scenario] = np.linspace(co2_values[0], final_co2, len(years))\n",
        "\n",
        "# --------------------------- Plot Trajectories with Annotations --------------------------- #\n",
        "plt.figure(figsize=(12,6))\n",
        "colors = [\"grey\", \"skyblue\", \"lightgreen\", \"lightcoral\", \"blue\", \"darkorange\", \"red\"]\n",
        "\n",
        "for scenario, color in zip(scenario_data[\"Scenario\"], colors):\n",
        "    plt.plot(years, trajectory_dict[scenario], label=scenario, color=color, lw=2)\n",
        "\n",
        "# Annotate top features on Aggressive decarbonisation scenario\n",
        "for feature, y_offset in zip(top_features, [0.3, 0.6, 0.9, 1.2]):\n",
        "    plt.text(2040, trajectory_dict[\"Aggressive decarbonisation\"][-1]+y_offset, feature,\n",
        "             color='black', fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.axhline(0, color='black', lw=1, linestyle='--', label=\"Net-Zero\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Projected COâ‚‚ Emissions (Mt)\")\n",
        "plt.title(\"Projected COâ‚‚ Emission Trajectories to 2050 with Key Drivers\")\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.legend(loc='upper right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eThZ6A5WPRwK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
